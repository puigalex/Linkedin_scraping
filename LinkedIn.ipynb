{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "import csv\n",
    "import logging\n",
    "from dotenv import load_dotenv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_logfile():\n",
    "    date_time = datetime.datetime.today().strftime('%d-%b-%y_%H:%M:%S')\n",
    "    logfile = f\"log/{date_time}.log\"\n",
    "    logging.basicConfig(filename=logfile, filemode='w', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', force=True)\n",
    "    logging.info(f'Log file {logfile} created')\n",
    "    return logging\n",
    "\n",
    "def create_file(file, logging):\n",
    "    # delete existing file if re-running\n",
    "    logging.info(\"Checking if current daily csv exists...\")\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        logging.info(f\"{file} deleted\")\n",
    "    else:\n",
    "        logging.info(f\"{file} ain't exist\")\n",
    "    \n",
    "    # create file and add header\n",
    "    logging.info(\"Creating daily csv file...\")\n",
    "    header = ['fecha', 'pais', 'busqueda', 'numero_resultados', 'titulo_vacantes']\n",
    "    with open(file, 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(header)\n",
    "        logging.info(f\"{file} created\")\n",
    "\n",
    "def login(logging):\n",
    "    url_login = \"https://www.linkedin.com/\"\n",
    "\n",
    "    # pulls login information from file called '.env' \n",
    "    # this file added to .gitignore so login details not shared\n",
    "    load_dotenv()\n",
    "    # .env file is of structure:\n",
    "    # LINKEDIN_USERNAME=email@gmail.com #Puedes poner su usuario y contraseña aqui en vez de .env (No es recomendable por temas de seguridad)\n",
    "    # LINKEDIN_PASSWORD=password\n",
    "\n",
    "    LINKEDIN_USERNAME = os.getenv('LINKEDIN_USERNAME')\n",
    "    LINKEDIN_PASSWORD = os.getenv('LINKEDIN_PASSWORD')\n",
    "\n",
    "    # setup chrome to run headless\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    # login to LinkedIn\n",
    "    logging.info(f\"Logging in to LinkedIn as {LINKEDIN_USERNAME}...\")\n",
    "    wd = webdriver.Chrome(executable_path='./chromedriver', options=chrome_options)\n",
    "    wd.get(url_login)\n",
    "    wd.find_element_by_id(\"session_key\").send_keys(LINKEDIN_USERNAME)\n",
    "    wd.find_element_by_id(\"session_password\").send_keys(LINKEDIN_PASSWORD)\n",
    "    wd.find_element(By.XPATH, \"//button[@class='sign-in-form__submit-button']\").click()\n",
    "\n",
    "    # random confirm acount information pop up that may come up\n",
    "    try: \n",
    "        wd.find_element(By.XPATH, \"//button[@class='primary-action-new']\").click()\n",
    "    except:\n",
    "        pass\n",
    "    logging.info(\"Log in complete. Scraping data...\")\n",
    "\n",
    "    return wd\n",
    "\n",
    "def page_search(wd, geo_id, search_location, search_keyword, search_remote, search_posted, search_page, search_count, file, logging):\n",
    "    # Segundos a esperar entre búsquedas\n",
    "    page_wait = 20\n",
    "    list_jobs = []\n",
    "\n",
    "    # when retrying, number of attempts\n",
    "    attempts = 3\n",
    "\n",
    "    # navigate to search page\n",
    "    url_search = f\"https://www.linkedin.com/jobs/search/?f_TPR={search_posted}&f_WRA={search_remote}&geoId={geo_id}&keywords={search_keyword}&location={search_location}&start={search_page}\"\n",
    "    # url_search = f\"https://www.linkedin.com/jobs/search/?f_TPR={search_posted}&geoId={geo_id}&keywords={search_keyword}&location={search_location}&start={search_page}\"\n",
    "    print(url_search)\n",
    "    wd.get(url_search)\n",
    "    time.sleep(page_wait) # add sleep so don't get caught\n",
    "\n",
    " \n",
    "    search_count = wd.find_element(By.XPATH, \"/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/header/div[1]/small\").text # //header/div[1]/small[1] /html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/header/div[1]/small\n",
    "    search_count = int(search_count.split(' ')[0].replace(',', '').replace(\".\", \"\"))  # get number before space & remove comma (ex. \"1,245 results\")\n",
    "    logging.info(f\"Loading page {round(search_page/25) + 1} of {round(search_count/25)} for {search_keyword}'s {search_count} results...\")\n",
    "    job_elements = wd.find_elements_by_class_name(\"job-card-list__title\")\n",
    "    job_titles = [title.text for title in job_elements]\n",
    "    print(\"Registro de cambio\", job_titles)\n",
    "    \n",
    "    date_time = datetime.datetime.now().strftime(\"%d%b%Y-%H:%M:%S\")\n",
    "    search_keyword = search_keyword.replace(\"%20\", \" \")\n",
    "    list_job = [date_time, search_location, search_keyword, search_count, job_titles]\n",
    "    list_jobs.append(list_job)\n",
    "\n",
    "    with open(file, \"a\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerows(list_jobs)\n",
    "        list_jobs = []\n",
    "    \n",
    "    logging.info(f\"Page {round(search_page/25) + 1} of {round(search_count/25)} loaded for {search_keyword}\")\n",
    "    search_page += 25   \n",
    "\n",
    "    # return search_page, search_count, url_search\n",
    "    return 1, 1, url_search\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create logging file\n",
    "logging = create_logfile()\n",
    "search_location = [\"Mexico\", \"Colombia\", \"Peru\", \"España\", \"Argentina\", \"Chile\", \"Ecuador\", \"Bolivia\", \"Guatemala\", \"Venezuela\", \"USA\", \"Remoto\"] \n",
    "location_dict = {\"España\": \"105646813\", \"Mexico\": \"103323778\", \"Colombia\": \"100876405\", \"Peru\": \"102927786\",\n",
    "                \"Argentina\": \"100446943\", \"Chile\": \"104621616\", \"Ecuador\": \"106373116\", \"Bolivia\": \"104379274\",\n",
    "                \"Guatemala\": \"100877388\", \"Venezuela\": \"101490751\", \"USA\": \"103644278\"}\n",
    "search_remote = \"true\" # filter for remote positions\n",
    "# search_posted = \"r86400\" # vacantes del ultimas 24 horas\n",
    "search_posted = \"r2592000\" # vacantes del ultimo mes\n",
    "\n",
    "# create daily csv file\n",
    "date = datetime.date.today().strftime('%d-%b-%y')\n",
    "file = f\"output/{date}_adicional.csv\"\n",
    "create_file(file, logging)\n",
    "\n",
    "# login to linkedin and assign webdriver to variable\n",
    "wd = login(logging)\n",
    "\n",
    "# URL search terms focusing on what type of skills are required for Data Analyst & Data Scientist\n",
    "search_keywords = ['tensorflow', \"pytorch\", \"python\", \"scikit\", \"numpy\", \"azure\", \"ing machine learning\", \"machine learning\", \"pandas\", \"docker\", \"aws\", \"deep learning\", \n",
    "                \"inteligencia artificial\", \"redes neuronales\", \"vision por computadora\", \"cv2\", \"open cv\", \"hugging face\", \"MLOps\", \"Data Science\",\n",
    "                \"GCP\", \"XGBoost\", \"NLP\", \"Chatbot\", \"Transformers\", \"GAN\", \"SQL\", \"Google cloud platform\"]\n",
    "\n",
    "# Counting Exceptions\n",
    "exception_first = 0\n",
    "exception_second = 0\n",
    "conteo = []\n",
    "for country in location_dict:\n",
    "    for search_keyword in search_keywords:\n",
    "        search_keyword = search_keyword.lower().replace(\" \", \"%20\")\n",
    "\n",
    "    # Loop through each page and write results to csv\n",
    "        search_page = 0 # start on page 1\n",
    "        search_count = 1 # initiate search count until looks on page\n",
    "        while (search_page < search_count) and (search_page != 1000 ):\n",
    "            # Search each page and return location after each completion\n",
    "            print(\"PRIMER INTENTO\")\n",
    "            try:\n",
    "                search_page, search_count, url_search = page_search(wd, location_dict[country], country, search_keyword, search_remote, search_posted, search_page, search_count, file, logging)\n",
    "                conteo.append(search_count)\n",
    "            except Exception as e:\n",
    "                logging.error(f'(1) FIRST exception for {search_keyword} on {search_page} of {search_count}, retrying...')\n",
    "                logging.error(f'Current URL: {url_search}')\n",
    "                logging.error(e)\n",
    "                logging.exception('Traceback ->')\n",
    "                exception_first += 1\n",
    "                time.sleep(5) \n",
    "                try:\n",
    "                    search_page, search_count, url_search = page_search(wd, location_dict[country], country, search_keyword, search_remote, search_posted, search_page, search_count, file, logging)\n",
    "                    logging.warning(f'Solved Exception for {search_keyword} on {search_page} of {search_count}')\n",
    "                except Exception as e:\n",
    "                    logging.error(f'(2) SECOND exception remains for {search_keyword}. Skipping to next page...')\n",
    "                    logging.error(f'Current URL: {url_search}')\n",
    "                    logging.error(e)\n",
    "                    logging.exception('Traceback ->')\n",
    "                    search_page += 25 # skip to next page to avoid entry\n",
    "                    exception_second += 1\n",
    "                    logging.error(f'Skipping to next page for {search_keyword}, on {search_page} of {search_count}...')\n",
    "\n",
    "# close browser\n",
    "wd.quit()\n",
    "\n",
    "logging.info(f'LinkedIn data scraping complete with {exception_first} first and {exception_second} second exceptions')\n",
    "logging.info(f'Regard all further alarms...')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be9c45174c02d7bb44899651cfca2677da1b0068869a12e57ca3f8a59e15c671"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.13 64-bit ('Linkedin': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}